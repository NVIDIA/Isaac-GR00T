# Environment variables for docker-compose.yml

# Port on which the inference server will run
INFERENCE_PORT=8000

# Path to the model on huggingface, if this is commented out, the model provided by the local path below is used during inference
HF_MODEL_PATH=nvidia/GR00T-N1.5-3B

# Local path to the fine-tuned model.
MODEL_HOST_PATH=/path/to/your/finetuned/local/model

# Name of the data configuration as specified in the gr00t/experiment/data_config.py file or in the respectively extended tng_rtc/extended_data_config.py
DATA_CONFIG=fourier_gr1_arms_only

# Tag of the embodiment used by the model
EMBODIMENT_TAG=gr1
